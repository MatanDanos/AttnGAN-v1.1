{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Learning\n",
    "Will be used in case the third generator experienced mode collapse and we want to restart it's weights.    \n",
    "The new weights will be saved in `manual_attngan.pt` and should be loaded when training restarts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataset.birds_dataset import BirdsDataset\n",
    "from models.discriminator import Discriminator\n",
    "from models.generator import GeneratorNetwork\n",
    "from models.conditional_augmentation import ConditioningAugmentation\n",
    "from models.text_encoder import TextEncoder\n",
    "from models.image_encoder import ImageEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder_weights_path = \"/home/user_2/AttnGAN/trained_weights/bird/image_encoder200.pth\"\n",
    "text_encoder_weights_path = \"/home/user_2/AttnGAN/trained_weights/bird/text_encoder200.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-16 15:33:51.169 | DEBUG    | models.image_encoder:__init__:23 - Started loading the Inception-v3 model\n",
      "2019-10-16 15:33:52.933 | DEBUG    | models.image_encoder:__init__:25 - Finished loading the Inception-v3 model\n"
     ]
    }
   ],
   "source": [
    "image_encoder = ImageEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Image Encoder Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder_stat_dict = torch.load(image_encoder_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder_stat_dict['emb_features.weight']\n",
    "# image_encoder_stat_dict['emb_cnn_code.weight']\n",
    "# image_encoder_stat_dict['emb_cnn_code.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign Image Encoder Weigts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder.fc_local.weight = torch.nn.Parameter(image_encoder_stat_dict['emb_features.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder.fc_global.weight = torch.nn.Parameter(image_encoder_stat_dict['emb_cnn_code.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder.fc_global.bias = torch.nn.Parameter(image_encoder_stat_dict['emb_cnn_code.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder = image_encoder.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 299, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = image_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_dataset = BirdsDataset(\"/home/user_2/AttnGAN/datasets/cub200-2011/preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4055"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_dataset.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Encoder Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_stat_dict = torch.load(text_encoder_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5450, 300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_stat_dict['encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-09e1c9844111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_encoder_stat_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "text_encoder_stat_dict['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign text encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(bird_dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.RNN.weight_ih_l0 = torch.nn.Parameter(text_encoder_stat_dict['rnn.weight_ih_l0'])\n",
    "text_encoder.RNN.weight_hh_l0 = torch.nn.Parameter(text_encoder_stat_dict['rnn.weight_hh_l0'])\n",
    "text_encoder.RNN.bias_ih_l0 = torch.nn.Parameter(text_encoder_stat_dict['rnn.bias_ih_l0'])\n",
    "text_encoder.RNN.bias_hh_l0 = torch.nn.Parameter(text_encoder_stat_dict['rnn.bias_hh_l0'])\n",
    "text_encoder.RNN.weight_ih_l0_reverse = torch.nn.Parameter(text_encoder_stat_dict['rnn.weight_ih_l0_reverse'])\n",
    "text_encoder.RNN.weight_hh_l0_reverse = torch.nn.Parameter(text_encoder_stat_dict['rnn.weight_hh_l0_reverse'])\n",
    "text_encoder.RNN.bias_ih_l0_reverse = torch.nn.Parameter(text_encoder_stat_dict['rnn.bias_ih_l0_reverse'])\n",
    "text_encoder.RNN.bias_hh_l0_reverse = torch.nn.Parameter(text_encoder_stat_dict['rnn.bias_hh_l0_reverse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"text_encoder\": text_encoder.state_dict(),\n",
    "            \"image_encoder\": image_encoder.state_dict()},\n",
    "          \"/home/user_2/AttnGAN/Matan/AttnGAN/models/best_encoders_weights.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring pre mode-collapse state for last G & D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generator import GeneratorNetwork\n",
    "from models.conditional_augmentation import ConditioningAugmentation\n",
    "from models.discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = GeneratorNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = Discriminator(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt2 = torch.optim.Adam(D2.parameters(), lr=0.00002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = torch.load(\"../models/attngan/epoch_checkpoint_attngan.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you want a random model\n",
    "tmp_model['models']['generator'] = gn.state_dict()\n",
    "tmp_model['models']['discriminators'][2] = D2.state_dict()\n",
    "tmp_model['optimizers']['D_optimizers'][2] = opt2.state_dict()\n",
    "torch.save(tmp_model, \"./tmp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_50 = torch.load(\"./tmp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = torch.load(\"../models/attngan/epoch_checkpoint_attngan.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_2_weights_names = [\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_1.0.weight\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_1.1.weight\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_1.1.bias\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_1.1.running_mean\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_1.1.running_var\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_1.1.num_batches_tracked\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_2.0.weight\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_2.1.weight\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_2.1.bias\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_2.1.running_mean\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_2.1.running_var\",\n",
    "\"stages_layers.2.residual_layer.0.residual_layer_2.1.num_batches_tracked\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_1.0.weight\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_1.1.weight\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_1.1.bias\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_1.1.running_mean\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_1.1.running_var\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_1.1.num_batches_tracked\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_2.0.weight\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_2.1.weight\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_2.1.bias\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_2.1.running_mean\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_2.1.running_var\",\n",
    "\"stages_layers.2.residual_layer.1.residual_layer_2.1.num_batches_tracked\",\n",
    "\"stages_layers.2.upsample.upsample_block.0.weight\",\n",
    "\"stages_layers.2.upsample.upsample_block.0.bias\",\n",
    "\"stages_layers.2.upsample.upsample_block.1.weight\",\n",
    "\"stages_layers.2.upsample.upsample_block.2.weight\",\n",
    "\"stages_layers.2.upsample.upsample_block.2.bias\",\n",
    "\"stages_layers.2.upsample.upsample_block.2.running_mean\",\n",
    "\"stages_layers.2.upsample.upsample_block.2.running_var\",\n",
    "\"stages_layers.2.upsample.upsample_block.2.num_batches_tracked\",\n",
    "\"attention_layers.1.fc.weight\",\n",
    "\"attention_layers.1.fc.bias\",\n",
    "\"image_generators.2.image.0.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the relevant info to \n",
    "for name in g_2_weights_names:\n",
    "    last_epoch['models']['generator'][name] = torch.nn.Parameter(epoch_50['models']['generator'][name], requires_grad=False)\n",
    "    if '.num_batches_tracked' not in name:\n",
    "        last_epoch['models']['generator'][name].requires_grad = True\n",
    "last_epoch['models']['discriminators'][2] = epoch_50['models']['discriminators'][2]\n",
    "last_epoch['optimizers']['D_optimizers'][2] = epoch_50['optimizers']['D_optimizers'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gn = GeneratorNetwork()\n",
    "gn.load_state_dict(last_epoch['models']['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca = ConditioningAugmentation()\n",
    "ca.load_state_dict(last_epoch['models']['conditional_augmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.FloatTensor of size 400x256]\n",
       "    (1): Parameter containing: [torch.FloatTensor of size 400]\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.FloatTensor of size 400x256]\n",
       "    (1): Parameter containing: [torch.FloatTensor of size 400]\n",
       "    (2): Parameter containing: [torch.FloatTensor of size 65536x200]\n",
       "    (3): Parameter containing: [torch.FloatTensor of size 65536]\n",
       "    (4): Parameter containing: [torch.FloatTensor of size 65536]\n",
       "    (5): Parameter containing: [torch.FloatTensor of size 2048x2048x3x3]\n",
       "    (6): Parameter containing: [torch.FloatTensor of size 2048]\n",
       "    (7): Parameter containing: [torch.FloatTensor of size 2048x2048x3x3]\n",
       "    (8): Parameter containing: [torch.FloatTensor of size 2048]\n",
       "    (9): Parameter containing: [torch.FloatTensor of size 2048]\n",
       "    (10): Parameter containing: [torch.FloatTensor of size 1024x1024x3x3]\n",
       "    (11): Parameter containing: [torch.FloatTensor of size 1024]\n",
       "    (12): Parameter containing: [torch.FloatTensor of size 1024x1024x3x3]\n",
       "    (13): Parameter containing: [torch.FloatTensor of size 1024]\n",
       "    (14): Parameter containing: [torch.FloatTensor of size 1024]\n",
       "    (15): Parameter containing: [torch.FloatTensor of size 512x512x3x3]\n",
       "    (16): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (17): Parameter containing: [torch.FloatTensor of size 512x512x3x3]\n",
       "    (18): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (19): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (20): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (21): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (22): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (23): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (24): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (25): Parameter containing: [torch.FloatTensor of size 512x256x3x3]\n",
       "    (26): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (27): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (28): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (29): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (30): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (31): Parameter containing: [torch.FloatTensor of size 512x256x3x3]\n",
       "    (32): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (33): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (34): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (35): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (36): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (37): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (38): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (39): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (40): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (41): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (42): Parameter containing: [torch.FloatTensor of size 512x256x3x3]\n",
       "    (43): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (44): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (45): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (46): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (47): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (48): Parameter containing: [torch.FloatTensor of size 512x256x3x3]\n",
       "    (49): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (50): Parameter containing: [torch.FloatTensor of size 512]\n",
       "    (51): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (52): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (53): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (54): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (55): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (56): Parameter containing: [torch.FloatTensor of size 256x256x3x3]\n",
       "    (57): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (58): Parameter containing: [torch.FloatTensor of size 256]\n",
       "    (59): Parameter containing: [torch.FloatTensor of size 128x256]\n",
       "    (60): Parameter containing: [torch.FloatTensor of size 128]\n",
       "    (61): Parameter containing: [torch.FloatTensor of size 128x256]\n",
       "    (62): Parameter containing: [torch.FloatTensor of size 128]\n",
       "    (63): Parameter containing: [torch.FloatTensor of size 3x128x3x3]\n",
       "    (64): Parameter containing: [torch.FloatTensor of size 3x128x3x3]\n",
       "    (65): Parameter containing: [torch.FloatTensor of size 3x128x3x3]\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_params = torch.nn.ParameterList()\n",
    "G_params.extend(ca.parameters())\n",
    "G_params.extend(gn.parameters())\n",
    "\n",
    "g_opt = torch.optim.Adam(G_params, lr=0.0002, betas=(0.5, 0.999))\n",
    "last_epoch['optimizers']['G_optimizer'] = g_opt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(last_epoch, '../models/attngan/manual_attngan.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
